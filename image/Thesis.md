# What is the environmental impact of large language models (LLMs), and how can these same models be leveraged to reduce global carbon footprints?

# Introduction English

Climate change is among the most pressing challenges facing the global community today, requiring immediate and coordinated responses across all sectors of society. Digital technology, notably artificial intelligence (AI), is simultaneously recognized as both a significant contributor to greenhouse gas emissions and as a powerful tool for addressing environmental challenges. Within the field of AI, Large Language Models (LLMs)—advanced computational models trained on vast datasets to understand and generate human-like text—have risen dramatically in prominence and application, influencing countless aspects of modern life, from digital communication to industrial optimization.

The environmental impact of these models, however, remains poorly understood by the broader public and even within many sectors of the technology industry. Their training and operation demand immense computational resources, translating into considerable energy consumption and, consequently, substantial carbon emissions. Yet paradoxically, these same technologies hold potential for significant positive environmental impacts, capable of optimizing energy use, enhancing sustainability initiatives, and supporting climate change mitigation efforts.

This thesis aims to critically analyze the dual role of Large Language Models in the context of climate change. Specifically, it addresses the central question: *"What is the environmental impact of large language models, and how can these same models be leveraged to reduce global carbon footprints?"* By systematically examining both the negative impacts associated with their development and operation, as well as their capacity for positive environmental contributions, this work seeks to offer a balanced and insightful perspective on the role of LLMs in contemporary environmental challenges. Ultimately, this exploration aims to inform sustainable practices within the field of AI, highlighting pathways for developers, policymakers, and corporations to responsibly harness the power of large-scale computational models for a more sustainable future.


# Introduction French

Le changement climatique est l'un des défis les plus urgents auxquels la communauté mondiale est confrontée aujourd'hui, nécessitant des réponses immédiates et coordonnées dans tous les secteurs de la société. Le numérique, notamment l'intelligence artificielle (IA), est reconnue à la fois comme un contributeur important aux émissions de gaz à effet de serre et comme un outil puissant pour relever les défis environnementaux. Dans le domaine de l'IA, les grands modèles de langage (LLM), des modèles informatiques avancés formés sur de vastes ensembles de données pour comprendre et générer des textes semblables à ceux de l'homme, ont considérablement gagné en importance et en application, influençant d'innombrables aspects de la vie moderne, de la communication numérique à l'optimisation industrielle.

L'impact environnemental de ces modèles reste cependant mal compris par le grand public et même dans de nombreux secteurs de l'industrie technologique. Leur apprentissage et leur fonctionnement nécessitent d'immenses ressources informatiques, ce qui se traduit par une consommation d'énergie considérable et, par conséquent, par des émissions de carbone importantes. Paradoxalement, ces mêmes technologies peuvent avoir des effets positifs importants sur l'environnement, en optimisant l'utilisation de l'énergie, en renforçant les initiatives de développement durable et en soutenant les efforts d'atténuation du changement climatique.

Cette thèse vise à analyser de manière critique le double rôle des grands modèles de langage dans le contexte du changement climatique. Plus précisément, elle aborde la question centrale : **"Quel est l'impact environnemental des grands modèles de langage et comment ces mêmes modèles peuvent-ils être utilisés pour réduire l'empreinte carbone mondiale ?"** En examinant systématiquement les impacts négatifs associés à leur développement et à leur fonctionnement, ainsi que leur capacité à contribuer positivement à l'environnement, ce travail cherche à offrir une perspective équilibrée et perspicace sur le rôle des modèles linguistiques à grande échelle dans les défis environnementaux contemporains. Finalement, cette thèse vise à informer les pratiques durables dans le domaine de l'IA, en mettant en évidence les voies permettant aux développeurs, aux décideurs politiques et aux entreprises d'exploiter de manière responsable la puissance des modèles de calcul à grande échelle pour un avenir plus durable.



# 2.1 Large Language Models (LLMs)

## Definition and Core Architecture: 

Large Language Models (LLMs) are a category of advanced AI language models distinguished by their immense scale and broad capabilities. They are neural network models characterized by hundreds of millions to billions of parameters, trained on massive corpora of text. Crucially, LLMs learn using a self-supervised training paradigm: they are first trained on vast amounts of unlabeled text (for example, by predicting missing or next words in sentences), which allows them to learn linguistic patterns without manual annotation. This pre-training is typically followed by task-specific fine-tuning on labeled data, a two-stage process that leverages general language knowledge and then adapts it to particular tasks. The core architecture enabling modern LLMs is the Transformer – an architecture introduced in 2017 that relies on a self-attention mechanism to model relationships between words in a sequence. Unlike earlier recurrent neural networks, the Transformer’s self-attention allows it to capture long-range context and dependencies in text more effectively and to be trained in parallel, which has been pivotal for scaling models to unprecedented sizes. In fact, the Transformer’s excellent parallelizability and capacity have made it the de facto backbone of today’s LLMs, making it feasible to build models with tens or even hundreds of billions of parameters. Prominent LLMs such as BERT, GPT and others all adopt the Transformer architecture at their core. Once trained, an LLM can generate human-like text by sequentially predicting the most probable next word, enabling it to produce coherent sentences and paragraphs in response to a given prompt.

## Evolution and Key Milestones: 
LLMs did not emerge in isolation but evolved from decades of progress in language modeling. Early language models in the late 20th century were mainly statistical n-gram models that predicted text based on a fixed-length context, but these had difficulty handling long-term context and complex language structure. The shift to neural network approaches in the 1980s and 90s (e.g. recurrent neural networks and the Long Short-Term Memory (LSTM) architecture) improved the ability to model sequential data, yet even these struggled with long-range dependencies in text. A major turning point came with the introduction of the Transformer architecture by (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin, *"Attention is all you Need"*), which overcame many limitations of RNN/LSTM models and enabled much deeper and larger networks. This breakthrough paved the way for a new generation of pre-trained language models. In 2018 Google researchers introduced BERT (Bidirectional Encoder Representations from Transformers), a large transformer-based model that demonstrated the power of pre-training on unlabeled text and fine-tuning for NLP tasks. Around the same time, OpenAI released the first Generative Pre-Trained Transformer (GPT) model. Subsequent milestones saw an explosive growth in model size and capabilities: GPT-2 (2019) contained 1.5 billion parameters and showed remarkably fluent text generation, and GPT-3 (2020) expanded to 175 billion parameters, exhibiting emergent abilities such as few-shot learning (the capacity to perform tasks with just a few examples or instructions) that were not observed in smaller models. The launch of user-friendly LLM-driven systems like ChatGPT in late 2022 further demonstrated the practical potential of LLMs and brought them into widespread public awareness. Throughout these developments, numerous other LLMs have been introduced by both academia and industry, for example, the T5 text-to-text model, XLNet, RoBERTa, and more recently large open-source models like GPT-Neo and LLaMA they collectively pushing the boundaries of language understanding and generation. This rapid technical evolution of LLMs is seen as transformative for AI at large, significantly influencing how AI systems are built and applied.

## Capabilities and Applications: 
One defining feature of LLMs is their general-purpose language ability. Thanks to training on diverse, Internet-scale data, LLMs acquire a broad knowledge base and the ability to perform a wide array of language tasks. They can comprehend context and generate human-like text, enabling applications such as text generation (e.g. writing essays or articles), summarization of documents, machine translation between languages, question-answering and conversational agents, and even reasoning over information given in textual form. In contrast to earlier specialized models, a single LLM can be adapted (through fine-tuning or prompting) to succeed in many different tasks, often with minimal task-specific training data, it is a property that has led researchers to dub them “foundation models” for AI applications. Moreover, at very large scales, LLMs have demonstrated emergent capabilities: for instance, they can perform in-context learning, meaning the model can learn to do a new task simply by being given instructions or examples in the prompt, without additional training, a behavior not seen in smaller models. This versatility makes LLMs powerful tools across numerous domains.

LLMs are already being applied across industry and academia, driving innovation in various sectors. For example, in healthcare, LLMs can assist in analyzing and summarizing medical records or literature, and even support diagnostic decision-making by interpreting clinical notes. In finance, LLMs are used to automate report generation, analyze financial news, and detect fraudulent transactions by understanding anomalous language patterns in communication. Education stands to benefit through intelligent tutoring systems and automated grading assistants that can evaluate or even personalize feedback on student writing. In the legal domain, LLMs can help in parsing and summarizing legal documents or in supporting legal research by quickly extracting relevant precedents from large text corpora. Customer service and e-commerce have embraced LLM-driven chatbots and virtual assistants to handle queries, provide product recommendations, and personalize the user experience. Even specialized fields like automotive engineering utilize LLMs for enhancing in-car voice assistants, enabling natural language navigation commands and real-time language translation for drivers.

Across these and other domains, LLMs serve as powerful engines for automation and insight, often matching or surpassing human-level performance on specialized language tasks. By leveraging their ability to understand context and generate relevant responses, organizations are using LLMs to streamline workflows, enhance customer interactions, and unlock data-driven insights in ways that were not previously possible.

## Toward Environmental Impact:

While LLMs offer remarkable capabilities, their development introduces significant sustainability challenges. The performance gains from scaling up model size come at the cost of extraordinarily high computational and energy requirements. Training a single large model (with billions of parameters) demands enormous processing power and electricity. For example, a 2019 analysis found that training one large language model could emit over 626,000 pounds of CO2 (equivalent to the lifetime emissions of five cars). More recently, training OpenAI’s GPT-3 (a 175-billion-parameter model) was estimated to consume about 1,287 MWh of electricity, producing roughly 500 metric tons of CO2 emissions. Even larger modern models (such as the more advanced GPT-4) presumably require comparable or greater resources, further exacerbating the carbon footprint of training.

Deploying and using LLMs at scale (serving millions of users) adds to ongoing energy and hardware demands. In fact, inference is the process of running the trained model for user queries that can account for a substantial share of overall energy usage. Each interaction with an LLM draws on power-intensive hardware; for instance, researchers estimate that a single ChatGPT query consumes about five times more electricity than a typical web search. Meeting the needs of countless such queries (and continuously fine-tuning models) means continuous electricity usage and cooling demands in data centers. These environmental and sustainability concerns have become a growing topic of research and debate. As we transition to the next section, we will examine the environmental impact of LLMs in more detail, considering how the pursuit of ever-larger models can be balanced with responsible and sustainable AI development.


# Section 2.2: Current Methods and Standards for Measuring the Environmental Impact of Computing

In this section, we review how the environmental footprint of computing is assessed, focusing on energy use and greenhouse gas (GHG) emissions. As Large Language Models (LLMs) demand massive computation, understanding these measurement methods is crucial to quantify their environmental impact. We cover how energy consumption is measured, how it is translated into CO₂-equivalent emissions, the tools and frameworks available, and the relevant standards (e.g. GHG Protocol, ISO 14064) for reporting such impacts. 

## 2.2.1 Measuring Energy Consumption in Computing

**Power Monitoring and Energy Use Metrics** The first step in assessing environmental impact is measuring the energy consumed by computing hardware. This is typically done by monitoring power draw (in watts) of CPUs, GPUs, memory, and other components over time Researchers often use on-board sensors or tools (e.g. NVIDIA’s nvidia-smi for GPUs, Intel’s RAPL for CPUs) to sample power usage during computation. By integrating power over the runtime of a task, one obtains the total energy usage in kilowatt-hours (kWh). For example, Strubell et al. measured GPU/CPU power during model training to estimate energy required for NLP models. Summing the energy of all devices involved (and accounting for any idle or overhead energy) provides the total electricity consumption of a computing task.

**Data Center Overhead – PUE** In large-scale computing (e.g. cloud data centers), not all drawn power goes to computation; cooling and infrastructure consume a share. A standard metric used is Power Usage Effectiveness (PUE), defined as the ratio of total data center power to IT equipment power. A PUE of 1.58 (the 2018 global average) means 58% extra energy overhead for cooling and facilities. Thus, to get the true energy footprint, the IT energy should be multiplied by PUE. For instance, if a server’s components consume 100 kWh, a PUE of 1.5 implies ~150 kWh were drawn from the grid in total. Incorporating PUE ensures we measure total energy used (including overhead) for computing operations.

**Life-Cycle Considerations** While operational energy is the main focus, a comprehensive assessment may also consider the embedded or embodied energy in hardware manufacturing and end-of-life. Life Cycle Assessment (LCA) frameworks (ISO 14040/14044) provide a holistic approach. An LCA of computing equipment accounts for the production phase (raw material extraction, chip fabrication, transport), the use phase (operational energy), and end-of-life (disposal/recycling). For example, studies have noted that manufacturing can contribute a large fraction of ICT carbon emissions (e.g. ~40% of a CPU-based data center’s GHG emissions came from production of equipment). However, obtaining precise hardware manufacturing data is challenging due to proprietary information. In practice, most current measurements focus on use-phase energy

## 2.2.2 Converting Energy Use to CO₂-Equivalent Emissions
**Emission Factors and CO₂e Calculation** Once energy consumption is known, it is converted into carbon dioxide-equivalent (CO₂e) emissions. This is done by multiplying the energy (in kWh) by an appropriate emissions factor (kg CO₂e per kWh). Emission factors depend on the energy source mix of the electricity used. For example, a grid predominantly using coal has a high emissions factor, whereas renewable energy yields a lower factor. In 2018 the global average was about 0.43 kg CO₂ per kWh (0.954 lb/kWh), but values vary by region and provider. Researchers use regional data or cloud-provider disclosures to select emission factors: e.g., Strubell et al. used U.S. averages and cloud-specific energy mixes to estimate carbon from training NLP models. If the computation is powered by a known mix (say 50% renewable, 50% gas), the weighted emission factor is applied. The formula integrating these elements is:
```math
\text{Emissions (CO₂e)} = \text{Energy (kWh)} \times \text{Emission Factor (kg CO₂e/kWh)}.
```
When accounting for data center overhead and multiple components, a more detailed formulation is used (from Stanford’s CS324 course ￼):
```math
\text{CO₂e} = R_{\text{power}\to\text{emit}} \times \text{PUE} \times \sum_{\text{devices}} (P_{\text{device}} \times t)
```
where 
```math
P_{\text{device}} 
```
is device power, t is runtime, and 
```math
R_{\text{power}\to\text{emit}}
```
 is the emission factor. This equation multiplies the energy used (power $\times$ time) by PUE and by the emission factor to yield CO₂ emissions.

![alt text](google-emissions-table.png)
*Google’s Emissions Factors Table, showing regional and energy source-specific factors.*

**Scope 1, 2, and 3 Emissions** In GHG accounting, emissions are categorized by scope under the Greenhouse Gas Protocol. Scope 1 covers direct emissions from owned sources (not usually applicable to electricity use in computing, but would include on-site generators, etc.), Scope 2 covers indirect emissions from purchased energy (electricity, steam, etc.), and Scope 3 covers other indirect emissions like supply chain and product use. For computing, the electricity used to run servers is a Scope 2 emission for an organization, whereas manufacturing of chips or cloud services purchased would fall under Scope 3 (as part of supply chain or outsourced activities). Current analyses of AI models primarily focus on Scope 2 emissions from electricity consumption during training and inference. Some recent studies attempt to include parts of Scope 3, such as embodied emissions of hardware or the downstream emissions from user devices during model inference, but these are harder to quantify. In summary, converting energy to CO₂e provides a common metric (usually reported in kilograms or tons of CO₂e) to compare environmental impacts.

![Screenshot 2025-07-07 at 12.12.34.png](<Screenshot 2025-07-07 at 12.12.34.png>)
*Schema of the GHG Protocol, showing Scope 1, 2 and 3 emissions.*

**Beyond Carbon** While CO₂ is the main focus due to climate change, other environmental impact metrics can be relevant. For instance, water usage is increasingly noted as an impact of computing; large AI model training can consume water for cooling and in electricity generation. Some frameworks now encourage reporting water and even electronic waste, though standards for these are less mature than for carbon. Still, the CO₂-equivalent remains the standard composite metric for GHG impact, often used as the primary indicator in computing sustainability studies.

## 2.2.3 Tools and Frameworks for Carbon Accounting in Computing

Researchers and organizations have developed various tools to measure and report the energy and carbon footprint of computing workloads:

**Software Instrumentation Libraries** One approach is to instrument code to monitor energy use in real time. CodeCarbon is a popular open-source Python library that tracks CPU/GPU energy consumption and estimates emissions based on the geographical location of the compute (using region-specific emission factors). It integrates with ML frameworks (TensorFlow, PyTorch) to provide fine-grained measurements, enabling developers to identify which part of a model pipeline is most energy-intensive. Such libraries use hardware power APIs and look-up tables of carbon intensity, simplifying the process for researchers to log the CO₂e emitted by their experiments. CodeCarbon is even part of the French Government toolbox for AI sustainability, showing its adoption in practice.

**Carbon Calculators** Tools like the ML CO2 Impact Calculator (by Victor Schmidt, Alexandra Luccioni, Alexandre Lacoste and Thomas Dandres) provide a user-friendly way to estimate emissions. Users input details such as the type of hardware (CPU/GPU model), total runtime, and cloud region or energy source, and the tool returns an approximate carbon footprint. 

![alt text](ariation-of-the-Average-Carbon-Intensity-of-Servers-Worldwide-by-Region-Vertical-bars.jpg)

*Variation of the Average Carbon Intensity of Servers Worldwide, by Region. (Vertical bars represent regions with a single available data point.)*

These calculators leverage published data on hardware TDP (thermal design power) and grid emission factors. They often output both raw emissions and “offset” emissions (accounting for any renewable energy credits from cloud providers). While convenient, their estimates rely on assumptions about average hardware utilization and may not capture dynamic power scaling, hence they are best used for rough estimates.

**Cloud Provider Sustainability Tools** Major cloud providers have introduced dashboards for clients to track the footprint of their cloud usage. For example, Microsoft Azure’s Sustainability Calculator (a Power BI application) helps organizations measure the carbon impact of their Azure cloud services. It aggregates a company’s cloud resource usage and applies Microsoft’s data on data center PUE and energy sourcing to estimate emissions. These tools leverage internal telemetry and “industry-standard emission factors” to give relatively accurate reports. Google and Amazon have similarly provided carbon footprint tools for their cloud platforms. Such tools are particularly useful for companies to track Scope 2 emissions of their cloud-based AI workloads as part of corporate GHG reporting (aligning with GHG Protocol and sustainability goals).

**Experiment Tracking and Reporting Frameworks** Within the AI research community, frameworks have emerged to encourage reporting of energy and emissions. For instance, Experiment Impact Tracker (Peter Henderson) is a toolkit that logs energy use of ML training runs and produces a report of carbon impact, integrating with training scripts. Some academic conferences now suggest or require submitting energy usage alongside model performance. The Green Software Foundation has even proposed a Software Carbon Intensity (SCI) score, which quantifies emissions per unit of software function. 

![alt text](01_carbon_efficiency-e5ce9cc2af3c13c4154ff61a0267b2b5.png)

## 2.2.4 Standards and Protocols for Environmental Impact Measurement

The field of computing sustainability borrows from general environmental accounting standards, ensuring that measurements are credible and comparable:

**ISO 14064 Series**  ISO 14064-1:2018 is an international standard that specifies principles and requirements for quantifying and reporting an organization’s GHG emissions and removals. It aligns closely with GHG Protocol and adds a layer of formal verification. Academic and industry studies on computing impact sometimes reference ISO 14064 to demonstrate rigor in carbon accounting. For instance, a white paper might claim compliance with ISO 14064 to ensure the carbon footprint of an AI service was quantified following accepted principles (e.g. completeness, transparency). The ISO 14064-2 standard focuses on project-level emissions reductions, and could be relevant if a new AI model or data center design is claimed to reduce emissions, it guides how to calculate that reduction. ISO 14064-3 covers third-party validation of emissions reports, which becomes important as companies publishing AI models may seek independent assurance of their environmental claims. In summary, ISO 14064 provides a structured, internationally recognized approach to measuring GHG emissions, and is the go-to standard for ensuring consistency and credibility in emission numbers.

**ISO 14040/14044 (Life Cycle Assessment Standards)** These ISO standards outline how to conduct a Life Cycle Assessment, which, as noted above, is crucial for evaluating environmental impacts beyond just electricity use. (*"Unraveling the Hidden Environmental Impacts of AI Solutions for Environment Life Cycle Assessment of AI Solutions"* by Anne-Laure Ligozat, Julien Lefèvre, Aurélie Bugeau and Jacques Combaz) emphasize using LCA to capture full impacts of ML models. The standards guide practitioners to define system boundaries, inventory all relevant emissions (from manufacturing to disposal), and assess multiple impact categories. In applying ISO 14040 to computing, one must consider the lifecycle of IT equipment and infrastructure supporting AI. While comprehensive, performing a full LCA for each new LLM is often impractical, but the standard serves as an aspirational framework to identify otherwise “hidden” impacts (like hardware production or coolant chemicals) that purely operational metrics might miss.

**Emerging Software-Specific Standards** Recognizing the need for IT-specific guidance, new standards are being developed. A notable example is ISO/IEC 21031:2024, a fresh standard focusing on the carbon footprint of software systems. It introduces the concept of a Software Carbon Intensity (SCI) score as a standardized measure of how much CO₂ is emitted per unit of software function. This standard essentially formalizes the methodology for calculating emissions attributable to software, covering steps like defining the software boundary (what parts of the software/hardware stack to include), measuring energy consumption, and determining the carbon intensity of the energy source. The SCI framework, developed in part by the Green Software Foundation, aligns with GHG Protocol/ISO 14064 principles but tailors them to software and cloud services. As AI applications (like LLM-driven services) are essentially software delivered at scale, ISO 21031 could soon provide a consistent way to report the carbon footprint per 1000 queries of an LLM service, for example. This represents a move toward standardizing “green software” metrics to complement traditional hardware-focused metrics.
	
**Other Relevant Metrics and Certifications** In practice, several industry metrics help evaluate computing efficiency which indirectly relate to environmental impact. For example, PUE (discussed above) is promoted by The Green Grid consortium and is widely adopted by data center operators as a key environmental efficiency metric. There are also certifications like ENERGY STAR, and data center sustainability certifications (LEED, ISO 50001 for energy management) which, while not specific to AI, ensure that infrastructure meets certain energy efficiency criteria. These standards and certifications create a context in which the environmental impact of LLMs should be interpreted – e.g., an AI lab running models in an ENERGY STAR-certified data center with a low PUE and 100% renewable energy procurement will have a much smaller GHG footprint than one using older, inefficient facilities.

## 2.2.5 Application of These Methods and Standards to AI and LLMs

Having outlined general methods, we now connect them to AI/LLM-specific scenarios. The energy and carbon footprint of Large Language Models are typically assessed at two key stages: **training and inference (deployment)**.

**Training Phase Footprint** Training a state-of-the-art LLM involves running tens of thousands of GPU hours, making energy usage enormous. Researchers apply the measurement techniques above to quantify this. For example, a landmark study by Emma Strubell, Ananya Ganesh, Andrew McCallum. (2019) measured the energy consumed in training several NLP models and estimated their CO₂ emissions. 

![alt text](image.png)

*Estimated cost of training a model in terms of CO2 emissions (lbs) and cloud compute cost (USD).7 Power and carbon footprint are omitted for TPUs due to lack of public information on power draw for this hardware.*

They found that training a large transformer with hyperparameter tuning emitted on the order of hundreds of thousands of pounds of CO₂ (over 280 metric tons), roughly equivalent to five cars’ lifetime emissions. The procedure included monitoring GPU power draw, summing total kWh, and multiplying by a carbon factor. More recent LLM training reports (for models like GPT-3, Llama, etc.) similarly calculate emissions by recording total energy used. Notably, Meta’s Llama model reports listed the electricity consumed and then converted to emissions using region-specific factors (though early versions used rough averages). In a comprehensive 2025 study, Jacob Morrison, Clara Na, Jared Fernandez, Tim Dettmers, Emma Strubell, Jesse Dodge. measured the training of a series of language models and explicitly followed GHG Protocol Scope 2 methods, determining that training their 13-billion-parameter model consumed hundreds of MWh and emitted 493 metric tons of CO₂e (assuming typical U.S. grid mix). These examples illustrate how standard energy-to-emission conversion is applied to quantify the climate impact of training LLMs.

![alt text](<Screenshot 2025-07-08 at 11.29.06.png>)

*(from ["Holistically Evaluating the Environmental Impact of Creating Language Models."])Models are ordered by their total water consumption and associated CO₂ emissions. Sub-billion-parameter systems were trained on 1.7 trillion tokens; OLMo 1B on 3 trillion; OLMo 2-7B on 4 trillion; OLMoE on 5 trillion; and OLMo 2-13B on 5.6 trillion. The data show that environmental impact rises sharply as both model size and training-data volume increase.*

**Inference and Deployment Footprint** Once deployed, LLMs can be used millions or billions of times, so the per-query energy becomes critical. Methods for measuring inference energy mirror those for training: instrument the model serving hardware to measure power per query, or estimate via benchmarked power usage. Some works use a functional unit like “per 1000 queries” to report emissions, aligning with the SCI approach of emissions per operation. For instance, an online inference energy tool by Hugging Face reports how many watt-hours each API call uses. By multiplying that by the carbon intensity of the host server’s electricity, one obtains CO₂e per query. A study by Alexandra Sasha Luccioni, Sylvain Viguier, Anne-Laure Ligozat. *"Estimating the carbon footprint of bloom, a 176B parameter language model"* evaluated the footprint of serving NLP models and emphasized considering the potentially vast number of inferences, which can quickly match or exceed training emissions. Indeed, if an LLM is very popular, the cumulative electricity for inference (across all user queries) can rival the training cost within months. This highlights the need to measure environmental impact across the model lifecycle, training (one-time, but intensive) and inference (continuous). Standards like the GHG Protocol would count both under the service provider’s Scope 2 emissions, and recent research encourages reporting both stages.

**Using Standards in Practice** The methodologies and standards described are not just theoretical, they are increasingly being adopted by AI practitioners. For example, when OpenAI or Google report on their models’ sustainability, they often cite compliance with carbon accounting standards (or at least use standard units and methods). The GHG Protocol’s scope definitions have been explicitly referenced in academic work to clarify what is included in AI emission calculations. By doing so, a paper can state it calculated emissions “in accordance with Scope 2 accounting”, signaling that only electricity use was counted and using a location-based emission factor from an authoritative source (e.g., EPA or IEA data for grid emissions). Similarly, if a study includes manufacturing impact of AI hardware, it may cite LCA standards or prior LCAs of semiconductors to estimate that portion. In short, the community is moving toward standardized reporting: for each new model, report energy (in MWh), carbon emissions (in tons CO₂e with method described), possibly water usage, and assumptions (PUE, emission factor, etc.). This mirrors how other industries report environmental impact and allows comparisons and tracking of improvements over time.
	
**Challenges and Evolving Practices** Despite the tools and standards available, measuring LLM environmental impact still faces challenges. One issue is transparency: many AI companies do not disclose full details (e.g., exact energy use, locations, hardware manufacturing data). This makes third-party estimates uncertain. Another challenge is incorporating Scope 3 emissions (like chip manufacturing) reliably, current studies often have to use proxy data or broad assumptions. Nonetheless, the trend in research is to be ever more comprehensive. The BLOOM language model effort in 2022 was noted for providing an extensive environmental impact appendix, covering training energy by region and even inferring the impact of model development (experiments before final training). Following that, the latest works (e.g., Morrison et al. 2025) measure not only training and inference, but also water consumption and encourage using renewable energy or better cooling to mitigate those impacts. 


































## Key Academic Sources and White Papers (with Annotations)
	•	Strubell et al. (2019) – “Energy and Policy Considerations for Deep Learning in NLP.” This influential study from UMass Amherst provides one of the first quantifications of the environmental cost of modern AI models ￼. The authors measured the GPU/CPU energy consumption for training several NLP models and then computed the CO₂ emissions using standard conversion factors. They famously found that a large Transformer with neural architecture search emitted ~626,000 lbs of CO₂ (284 metric tons) ￼. This paper is relevant as it exemplifies how to apply energy measurement tools and emission calculations to AI, and it argues for reporting energy costs in academic research ￼. It establishes baseline methods (power monitoring, use of average emission factors) that are built upon by later works.
	•	Lannelongue et al. (2021) – “Green Algorithms: Quantifying the Carbon Footprint of Computation.” An academic article (Advanced Science) led by researchers at Cambridge University, it introduces a general framework and tool for calculating the carbon footprint of any computational task ￼ ￼. The authors distilled the problem to a few key parameters: hardware type, runtime, and location (energy grid mix), and provided an open-source online calculator ￼. The paper is highly relevant as it presents a standardized method to estimate energy and emissions, which can be directly applied to AI workloads. It also discusses the importance of transparency and includes strategies to reduce carbon footprint (e.g. using more efficient hardware or greener energy) ￼ ￼. This source demonstrates an academic approach to codifying measurement practices and has influenced subsequent tools like CodeCarbon.
	•	Wu et al. (2022) – “Sustainable AI: Environmental Implications, Challenges and Opportunities.” A comprehensive survey paper (from Facebook AI Research, presented at MLSys 2022) that examines the environmental impact of AI from a holistic perspective ￼ ￼. It spans the entire machine learning lifecycle – data, model training, hardware – and explicitly characterizes the carbon footprint of AI computing, including operational and manufacturing emissions ￼. The paper’s relevance lies in its discussion of measurement challenges: it highlights the need for better data on hardware manufacturing and advocates for industry-wide adoption of measurement standards. It also provides insights into optimizing hardware/software for efficiency and calls out the role of lifecycle assessment by examining hardware production and end-of-life ￼ ￼. In our context, this source supports the view that measuring LLM impact requires a broad systems approach and validates the importance of standards like LCA and holistic reporting.
	•	Morrison et al. (2025) – “Holistically Evaluating the Environmental Impact of Creating Language Models.” A recent (2025) study by researchers from Allen Institute for AI and CMU (including Emma Strubell) that sets a new bar for detailed LLM impact analysis. This work is notable for applying GHG Protocol standards (Scope 2) in calculating emissions for training a series of LLMs ￼, and for going further to measure previously under-reported aspects: development-phase energy, embodied carbon of hardware, and water usage ￼ ￼. They reported that even training relatively small LLMs (up to 13B parameters) in a efficient data center resulted in ~493 tCO₂ and consumed 2.8 million liters of water ￼. The paper’s methodology section (using power meters at sub-second intervals, region-specific emission factors, etc.) exemplifies current best practices ￼. This source is highly relevant as it demonstrates how the methods and standards discussed in Section 2.2 are concretely implemented to produce a thorough environmental assessment of LLMs, reinforcing the section’s points about comprehensive measurement and transparency.
	•	OECD (2022) – “Measuring the Environmental Impacts of AI: The AI Footprint.” An OECD policy paper that, while not solely academic, aggregates research and proposes directions for standardizing AI impact measurement ￼ ￼. It distinguishes direct impacts (compute energy, e-waste) from indirect effects and recommends the establishment of measurement standards specific to AI ￼. This report is included for its authoritative overview and its emphasis on looking beyond operational emissions (e.g., considering lifecycle and indirect impacts) ￼. It supports the thesis section by providing a high-level validation that measuring AI’s environmental impact is a recognized priority, and it cites many of the academic works above as evidence. The OECD’s recommendations underscore why the methods and standards in this outline are critical – to enable policymakers and stakeholders to reliably track the sustainability of AI development.
Sources:
	1.	Vaswani, A., et al. (2017). Attention is All You Need. Advances in Neural Information Processing Systems.
	2.	Devlin, J., et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL.
	3.	Brown, T., et al. (2020). Language Models are Few-Shot Learners. NeurIPS.
	4.	Zhao, W. X., et al. (2023). A Survey of Large Language Models. arXiv preprint arXiv:2303.18223 ￼ ￼.
	5.	Khan, A., et al. (2025). Industrial Applications of Large Language Models. Scientific Reports 15, 12345 ￼ ￼.
	6.	Bommasani, R., et al. (2021). On the Opportunities and Risks of Foundation Models. Stanford CRFM.
	7.	MIT News (2025). Explained: Generative AI’s environmental impact ￼.